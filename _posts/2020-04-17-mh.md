---
layout: post
title: Why Metropolis-Hastings Works
modified: 3/29/2020, 9:20:12
excerpt: "A gentle explanation"
comments: true
category: blog
---

This spring, I took a Bayesian statistics class in BU's biostats department (I turned in my final paper today!). If you've taken a class on Bayesian stats, you probably remember that it is very difficult to directly sample from posterior distributions, besides in a few special cases (i.e. conjugare priors). Thankfully, there are now a bunch of Markov Chain Monte Carlo (MCMC) methods that allow us to approximate posterior distributions very well. 

My class was mostly focused on topics/applications, so we breezed over MCMC approaches pretty quickly and I never fully understood why the software I was using to implement these algorithms was spitting out pretty reliable posterior estimates. I decided to go back and get the intuition behind one of the most general MCMC algorithms: the Metropolis-Hastings (MH) sampler. 

It took me a while to fully get the intuition. A lot longer than I was hoping for. Over the many explanations of the MH that I've seen, there are basically two types. The first type, typically presented by authors in the data science/machine learning fields, tends to just present the algorithm without justification, maybe with a coded up example. The second type, typically found in grad school notes/textbooks, was a bit too technical for me and I found myself getting tangled in a bunch of definitions/theorems/proofs.

Oddly enough, I've rarely seen authors describe the MH algorithm with any amount of praise beyond maybe calling it "clever." The idea that we can sample from a very-hard-if-not-impossible-to-sample-from by taking enough draws from a Markov Chain (which we never explcitly construct -- we don't save a transition kernel to memory during the algorithm) is absolutely nuts. There is far from an obvious result, and it requires quite a bit of rigor to fully grasp why it works. To that end, I'll try to explain the algorithm gently enough to keep you from losing your intuition while trying to keep up with a bunch of equations, but with just enough rigor so that you walk away understanding the key elements of the algorithm that make it work. 

## The problem we need to solve 

Suppose that there is some distribution $\tilde{p}(z) = \frac{1}{Z_p}p(z)$ with the following properties:

* $Z_p$ is really difficult to compute (sound familiar?)
* We can easy compute $p(z)$ for a given $z$, but we aren't able to sample from $p$ directly.

We want to find a roundabout way to sample from $p(z)$.

## The solution (AKA the algorithm)

Okay, so we have a distribution $\tilde{p}(z) = \frac{1}{Z_p}p(z)$ that we want to sample from. As we know, we want to construct a Markov Chain that has stationary distribution $\tilde{p}$. The algorithm is as follows 

1. Pick some initial starting point $z_0$ and set the current value of the chain $z_t=z_0$.

2. Define some "hopping" matrix $H$ (you get to choose it)   that functions as a sort-of transition matrix (you'll see      what I mean).

3. Define $z \equiv z_t$, the current value of the chain. Generate $\tilde{z}$ from the sort-of-transition matrix $H(\tilde{z},z)$. To do this, you would draw a value $x$ from a Uniform (0,1), find the current row of the sort-of-transition matrix and see where $x$ puts you in the CDF for that row.

4. Set $z_{t+1} = \tilde{z}$ with probability $\alpha( \tilde{z} | z ) = \min \left{1, \frac{p(\tilde{z})}{ p(z) } \frac{H(z | \tilde{z})}{H(\tilde{z} | z)} \right}$, and $z_{t+1} = z$ with probability $1-\alpha(\tilde{z} | z)$.

5. Repeat steps 4 and 5 many, many times.

As has been advertised many times by now, under certain regularity conditions the resulting sequence $\{z_1,z_2,...,z_N \}$ is a Markov Chain with stationary distribution $\tilde{p}$.

## Why does Metropolis-Hastings work?

From the Markov Chain section, you may remember that if a Markov Chain is time-reversible, then the "reversing distribution" is that Markov Chain's stationary distribution. This is pretty easy to prove:

**Proposition:** *Let $P$ be a transition matrix for a Markov Chain on an event space $\Omega$ and $\pi$ be a distribution that satisfies that time-reversing balance equation*

$$
P(y|x) \pi(x) = P(x|y) \pi(y)
$$

*Then, $\pi$ is the stationary distribution of $P$*.

Showing this is pretty easy. Let $y$ and $x$ be arbitrary elements of the event space $\Omega$. Then, we have

$$
\begin{align}
\sum_x P(y|x) \pi(x) & = \sum_x P(x|y) \pi(y) \\ 
\sum_x P(y|x) \pi(x) & = \pi(y) \sum_x P(x|yw) \\
\sum_x P(y|x) \pi(x) & = \pi(y)
\end{align}
$$

As this holds for all $x$ and $y$ on $\Omega$, we have that $P \pi = \pi$, which is the definition of a stationary distrbution. We're done!
  
Thus, if we can show that the algorithm above produces a reversible Markov Chain with "reversing distribution" $\tilde{p}$, the we know that we have produced a Markov Chain with stationary distribution $\tilde{p}$. Turns out, this is also easy to show.

Given our distribution of interest $\tilde{p}$, acceptance rule function $\alpha$ and "hopping" matrix $H$, we have the transition matrix $ P(y | x) = \alpha(y | x) H(y | x)$.
We want to show that our distribution of interest $\tilde{p}$ satisfies the time-reversing balancing equation.

We can write

$$
\begin{align}
\underbrace{\alpha(y|x) H(y|x)}_{P(y|x)} \tilde{p}(x) & = \min \{1, \frac{p(y) H(x|y)}{p(x) H(y|x)} \} H(y|x) \tilde{p}(x) \\
& = \min \{H(y|x) \tilde{p}(x), \tilde{p}(y) H(x|y) \} \\
& = \min \{ \frac{H(y|x) \tilde{p}(x)}{H(x|y) \tilde{p}(y)}, 1 \} H(x|y) \tilde{p}(y) \\
& = \underbrace{\alpha(x|y) H(x|y)}_{P(x|y)} \tilde{p}(y)
\end{align}
$$